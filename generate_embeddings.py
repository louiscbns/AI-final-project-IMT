import json
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import logging
import os
from sklearn.preprocessing import StandardScaler

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_modified_metadata(file_path):
    """
    Charge les m√©tadonn√©es modifi√©es depuis le fichier JSON
    """
    print("\n" + "="*50)
    print("√âTAPE 1: Chargement des m√©tadonn√©es")
    print("="*50)
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
            print(f"‚úÖ Donn√©es charg√©es avec succ√®s: {len(data)} plats trouv√©s")
            return data
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des m√©tadonn√©es : {e}")
        logger.error(f"Erreur lors du chargement des m√©tadonn√©es : {e}")
        return None

def generate_text_embeddings_transformer(texts, model_name="bert-base-uncased"):
    """
    G√©n√®re des embeddings pour une liste de textes en utilisant BERT
    """
    print("\n" + "="*50)
    print("√âTAPE 2: G√©n√©ration des embeddings")
    print("="*50)
    print(f"üìù Nombre de textes √† traiter: {len(texts)}")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üíª Utilisation du device: {device}")
    
    try:
        print("üîÑ Chargement du mod√®le BERT...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        print("‚úÖ Mod√®le BERT charg√© avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        logger.error(f"Erreur lors du chargement du mod√®le : {e}")
        return None

    print("üîÑ Tokenization des textes...")
    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    print("‚úÖ Tokenization termin√©e")

    print("üîÑ G√©n√©ration des embeddings...")
    with torch.no_grad():
        outputs = model(**encoded_input)
        text_features = outputs.last_hidden_state.mean(dim=1)
        text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # Convertir en numpy pour l'affichage
    text_features_np = text_features.cpu().numpy()
    
    print("\nüìä Informations sur les embeddings g√©n√©r√©s:")
    print(f"   - Nombre total d'embeddings: {len(text_features_np)}")
    print(f"   - Dimension de chaque embedding: {text_features_np.shape[1]}")
    
    # Afficher les statistiques pour chaque embedding
    print("\nüìà Statistiques par embedding:")
    for i in range(len(text_features_np)):
        embedding = text_features_np[i]
        print(f"\nEmbedding {i+1}:")
        print(f"   - Moyenne: {np.mean(embedding):.4f}")
        print(f"   - √âcart-type: {np.std(embedding):.4f}")
        print(f"   - Min: {np.min(embedding):.4f}")
        print(f"   - Max: {np.max(embedding):.4f}")
        print(f"   - Norme L2: {np.linalg.norm(embedding):.4f}")
        
        # Afficher les 5 premi√®res dimensions
        print(f"   - 5 premi√®res dimensions: {embedding[:5]}")
        
        # Afficher un extrait du texte correspondant
        print(f"   - Texte: {texts[i][:100]}...")
    
    print("\n‚úÖ Embeddings g√©n√©r√©s avec succ√®s")
    return text_features_np

def normalize_labels(y, output_dir='food-nutrients'):
    """
    Normalise les labels en utilisant StandardScaler
    """
    scaler = StandardScaler()
    y_normalized = scaler.fit_transform(y)
    
    # Cr√©er le r√©pertoire de sortie s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder les param√®tres de normalisation
    np.save(os.path.join(output_dir, 'scaler_mean.npy'), scaler.mean_)
    np.save(os.path.join(output_dir, 'scaler_scale.npy'), scaler.scale_)
    
    return y_normalized

def prepare_data(metadata):
    """
    Pr√©pare les donn√©es pour la g√©n√©ration des embeddings
    """
    print("\n" + "="*50)
    print("√âTAPE 3: Pr√©paration des donn√©es")
    print("="*50)
    
    X = []  # Textes pour les embeddings
    y = []  # Labels (calories, prot√©ines, etc.)
    ids = []  # IDs des plats
    
    print("üîÑ Extraction des descriptions et caract√©ristiques nutritionnelles...")
    total_plats = len(metadata)
    print(f"üìä Nombre total de plats √† traiter: {total_plats}")
    
    for i, dish in enumerate(metadata):
        text = dish.get('description', '')
        if text:
            X.append(text)
            features = [
                dish.get('total_calories', 0),
                dish.get('total_protein', 0),
                dish.get('total_carb', 0),
                dish.get('total_fat', 0)
            ]
            y.append(features)
            ids.append(dish.get('id', f'unknown_{i}'))
            
            # Afficher les informations tous les 100 plats
            if (i + 1) % 100 == 0:
                print(f"\nProgression: {i+1}/{total_plats} plats trait√©s ({(i+1)/total_plats*100:.1f}%)")
                print(f"Plat {i+1}:")
                print(f"Description: {text[:100]}...")
                print(f"Caract√©ristiques: Calories={features[0]}, Prot√©ines={features[1]}, Glucides={features[2]}, Lipides={features[3]}")
    
    print(f"\n‚úÖ Donn√©es pr√©par√©es: {len(X)} √©chantillons")
    
    # Convertir en array numpy et normaliser les labels
    y_array = np.array(y)
    y_normalized = normalize_labels(y_array, output_dir='food-nutrients')
    
    print(f"üìä Statistiques des caract√©ristiques nutritionnelles (apr√®s normalisation):")
    for i, feature_name in enumerate(['Calories', 'Prot√©ines', 'Glucides', 'Lipides']):
        print(f"   {feature_name}:")
        print(f"      - Moyenne: {np.mean(y_normalized[:, i]):.2f}")
        print(f"      - √âcart-type: {np.std(y_normalized[:, i]):.2f}")
        print(f"      - Min: {np.min(y_normalized[:, i]):.2f}")
        print(f"      - Max: {np.max(y_normalized[:, i]):.2f}")
    
    return X, y_normalized, ids

def save_data(X_embeddings, y, output_dir='food-nutrients'):
    """
    Sauvegarde les embeddings et les labels dans un seul fichier JSON
    """
    print("\n" + "="*50)
    print("√âTAPE 4: Sauvegarde des donn√©es")
    print("="*50)
    
    # Cr√©er le r√©pertoire de sortie s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)
    
    # Pr√©parer les donn√©es pour la sauvegarde
    data_dict = {
        'embeddings': X_embeddings.tolist(),
        'labels': y.tolist(),
        'metadata': {
            'embeddings_shape': X_embeddings.shape,
            'embeddings_dtype': str(X_embeddings.dtype),
            'labels_shape': y.shape,
            'labels_dtype': str(y.dtype),
            'feature_names': ['calories', 'proteines', 'glucides', 'lipides']
        }
    }
    
    # Sauvegarder toutes les donn√©es dans un seul fichier JSON
    with open(os.path.join(output_dir, 'model_data.json'), 'w') as f:
        json.dump(data_dict, f)
    print("‚úÖ Donn√©es sauvegard√©es dans 'food-nutrients/model_data.json'")

def main():
    print("\nüöÄ D√©marrage de la g√©n√©ration des embeddings")
    
    # Charger les m√©tadonn√©es modifi√©es
    metadata = load_modified_metadata('food-nutrients/metadata_modified.json')
    if metadata is None:
        return
    
    # Pr√©parer les donn√©es
    X, y, _ = prepare_data(metadata)
    
    # G√©n√©rer les embeddings
    X_embeddings = generate_text_embeddings_transformer(X)
    
    if X_embeddings is not None:
        # Sauvegarder les donn√©es
        save_data(X_embeddings, y)
    else:
        print("‚ùå √âchec de la g√©n√©ration des embeddings")

if __name__ == "__main__":
    main() 