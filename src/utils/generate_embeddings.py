import json
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import logging
import os
from sklearn.preprocessing import StandardScaler
from typing import Optional, List, Tuple, Dict, Any

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_modified_metadata(file_path: str) -> Optional[List[Dict[str, Any]]]:
    """
    Charge les m√©tadonn√©es modifi√©es depuis le fichier JSON.
    
    Args:
        file_path (str): Chemin vers le fichier JSON
    
    Returns:
        Optional[List[Dict[str, Any]]]: Donn√©es charg√©es ou None en cas d'erreur
    """
    print("\n" + "="*50)
    print("√âTAPE 1: Chargement des m√©tadonn√©es")
    print("="*50)
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
            print(f"‚úÖ Donn√©es charg√©es avec succ√®s: {len(data)} plats trouv√©s")
            return data
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des m√©tadonn√©es : {e}")
        logger.error(f"Erreur lors du chargement des m√©tadonn√©es : {e}")
        return None

def generate_text_embeddings_transformer(texts: List[str], model_name: str = "bert-base-uncased", batch_size: int = 32) -> Optional[np.ndarray]:
    """
    G√©n√®re des embeddings pour une liste de textes en utilisant BERT.
    
    Args:
        texts (List[str]): Liste des textes √† traiter
        model_name (str): Nom du mod√®le BERT √† utiliser
        batch_size (int): Taille des lots pour le traitement
    
    Returns:
        Optional[np.ndarray]: Embeddings g√©n√©r√©s ou None en cas d'erreur
    """
    print("\n" + "="*50)
    print("√âTAPE 2: G√©n√©ration des embeddings")
    print("="*50)
    print(f"üìù Nombre de textes √† traiter: {len(texts)}")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üíª Utilisation du device: {device}")

    try:
        print("üîÑ Chargement du mod√®le BERT...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name).to(device)
        print("‚úÖ Mod√®le BERT charg√© avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        logger.error(f"Erreur lors du chargement du mod√®le : {e}")
        return None

    all_embeddings = []
    total_texts = len(texts)

    for i in range(0, total_texts, batch_size):
        batch_texts = texts[i:i + batch_size]
        print(f"üîÑ Traitement du lot {i//batch_size + 1}/{(total_texts + batch_size - 1)//batch_size}...")

        print("üîÑ Tokenization du lot...")
        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')
        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
        print("‚úÖ Tokenization du lot termin√©e")

        print("üîÑ G√©n√©ration des embeddings du lot...")
        with torch.no_grad():
            outputs = model(**encoded_input)
            batch_features = outputs.last_hidden_state.mean(dim=1)
            batch_features /= batch_features.norm(dim=-1, keepdim=True)

        all_embeddings.append(batch_features.cpu().numpy())
        print("‚úÖ Embeddings du lot g√©n√©r√©s avec succ√®s")

        # Lib√©rer la m√©moire
        del encoded_input, outputs, batch_features
        if device == "cuda":
            torch.cuda.empty_cache()

    text_features_np = np.concatenate(all_embeddings, axis=0)

    print("\nüìä Informations sur les embeddings g√©n√©r√©s:")
    print(f"   - Nombre total d'embeddings: {len(text_features_np)}")
    print(f"   - Dimension de chaque embedding: {text_features_np.shape[1]}")

    return text_features_np

def normalize_labels(y: np.ndarray, output_dir: str) -> np.ndarray:
    """
    Normalise les labels en utilisant StandardScaler.
    
    Args:
        y (np.ndarray): Labels √† normaliser
        output_dir (str): R√©pertoire de sortie pour les param√®tres de normalisation
    
    Returns:
        np.ndarray: Labels normalis√©s
    """
    scaler = StandardScaler()
    y_normalized = scaler.fit_transform(y)

    # Cr√©er le r√©pertoire de sortie s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)

    # Sauvegarder les param√®tres de normalisation
    np.save(os.path.join(output_dir, 'scaler_mean.npy'), scaler.mean_)
    np.save(os.path.join(output_dir, 'scaler_scale.npy'), scaler.scale_)

    return y_normalized

def prepare_data(metadata: List[Dict[str, Any]], output_dir: str) -> Tuple[List[str], np.ndarray, List[str]]:
    """
    Pr√©pare les donn√©es pour la g√©n√©ration des embeddings.
    
    Args:
        metadata (List[Dict[str, Any]]): M√©tadonn√©es des plats
        output_dir (str): R√©pertoire de sortie
    
    Returns:
        Tuple[List[str], np.ndarray, List[str]]: (textes, labels normalis√©s, ids)
    """
    print("\n" + "="*50)
    print("√âTAPE 3: Pr√©paration des donn√©es")
    print("="*50)

    X = []  # Textes pour les embeddings
    y = []  # Labels (calories, prot√©ines, etc.)
    ids = []  # IDs des plats

    print("üîÑ Extraction des descriptions et caract√©ristiques nutritionnelles...")
    total_plats = len(metadata)
    print(f"üìä Nombre total de plats √† traiter: {total_plats}")

    for i, dish in enumerate(metadata):
        text = dish.get('description', '')
        if text:
            X.append(text)
            features = [
                dish.get('total_calories', 0),
                dish.get('total_protein', 0),
                dish.get('total_carb', 0),
                dish.get('total_fat', 0)
            ]
            y.append(features)
            ids.append(dish.get('id', f'unknown_{i}'))

            if (i + 1) % 100 == 0:
                print(f"\nProgression: {i+1}/{total_plats} plats trait√©s ({(i+1)/total_plats*100:.1f}%)")
                print(f"Plat {i+1}:")
                print(f"Description: {text[:100]}...")
                print(f"Caract√©ristiques: Calories={features[0]}, Prot√©ines={features[1]}, Glucides={features[2]}, Lipides={features[3]}")

    print(f"\n‚úÖ Donn√©es pr√©par√©es: {len(X)} √©chantillons")

    # Convertir en array numpy et normaliser les labels
    y_array = np.array(y)
    y_normalized = normalize_labels(y_array, output_dir=output_dir)

    print(f"üìä Statistiques des caract√©ristiques nutritionnelles (apr√®s normalisation):")
    for i, feature_name in enumerate(['Calories', 'Prot√©ines', 'Glucides', 'Lipides']):
        print(f"   {feature_name}:")
        print(f"      - Moyenne: {np.mean(y_normalized[:, i]):.2f}")
        print(f"      - √âcart-type: {np.std(y_normalized[:, i]):.2f}")
        print(f"      - Min: {np.min(y_normalized[:, i]):.2f}")
        print(f"      - Max: {np.max(y_normalized[:, i]):.2f}")

    return X, y_normalized, ids

def save_data(X_embeddings: np.ndarray, y: np.ndarray, output_dir: str) -> None:
    """
    Sauvegarde les embeddings et les labels dans un fichier JSON.
    
    Args:
        X_embeddings (np.ndarray): Embeddings g√©n√©r√©s
        y (np.ndarray): Labels normalis√©s
        output_dir (str): R√©pertoire de sortie
    """
    print("\n" + "="*50)
    print("√âTAPE 4: Sauvegarde des donn√©es")
    print("="*50)

    # Cr√©er le r√©pertoire de sortie s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)

    # Pr√©parer les donn√©es pour la sauvegarde
    data_dict = {
        'embeddings': X_embeddings.tolist(),
        'labels': y.tolist(),
        'metadata': {
            'embeddings_shape': X_embeddings.shape,
            'embeddings_dtype': str(X_embeddings.dtype),
            'labels_shape': y.shape,
            'labels_dtype': str(y.dtype),
            'feature_names': ['calories', 'proteines', 'glucides', 'lipides']
        }
    }

    # Sauvegarder toutes les donn√©es dans un seul fichier JSON
    output_file = os.path.join(output_dir, 'model_data.json')
    with open(output_file, 'w') as f:
        json.dump(data_dict, f, indent=4)
    print(f"‚úÖ Donn√©es sauvegard√©es dans '{output_file}'")

def main():
    """
    Fonction principale qui ex√©cute le processus complet de g√©n√©ration des embeddings.
    """
    print("\nüöÄ D√©marrage de la g√©n√©ration des embeddings")
    
    output_dir = 'food-nutrients'
    
    # Charger les m√©tadonn√©es modifi√©es
    metadata = load_modified_metadata(os.path.join(output_dir, 'metadata_modified.json'))
    if metadata is None:
        return
    
    # Pr√©parer les donn√©es
    X, y, _ = prepare_data(metadata, output_dir)
    
    # G√©n√©rer les embeddings
    X_embeddings = generate_text_embeddings_transformer(X)
    
    if X_embeddings is not None:
        # Sauvegarder les donn√©es
        save_data(X_embeddings, y, output_dir)
    else:
        print("‚ùå √âchec de la g√©n√©ration des embeddings")

if __name__ == "__main__":
    main() 